import { ModelData } from "@/components/Leaderboard";

export const scoresData: ModelData[] = [
  {
    model: "gpt-4o-mini-2024-07-18",
    Size: "-",
    CNN: 61.8,
    XSum: 73.6,
    MediaS: 71.3,
    MeetB: 79.7,
    WiCE: 76.3,
    REVEAL: 85.8,
    ClaimVerify: 69.8,
    FactCheck: 76.0,
    ExpertQA: 58.3,
    LFQA: 80.3,
    RAGTruth: 81.6,
    color: "#ecfccb",
    link: "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
  },
  {
    model: "gpt-4o-2024-05-13",
    Size: "-",
    CNN: 68.1,
    XSum: 76.8,
    MediaS: 71.4,
    MeetB: 79.8,
    WiCE: 78.5,
    REVEAL: 86.5,
    ClaimVerify: 69.0,
    FactCheck: 77.5,
    ExpertQA: 59.6,
    LFQA: 83.6,
    RAGTruth: 84.3,
    color: "#bef264",
    link: "https://platform.openai.com/docs/models/gpt-4o",
  },
  {
    model: "gpt-4-turbo-preview",
    Size: "-",
    CNN: 66.7,
    XSum: 76.5,
    MediaS: 71.4,
    MeetB: 79.9,
    WiCE: 80.4,
    REVEAL: 87.8,
    ClaimVerify: 67.6,
    FactCheck: 79.9,
    ExpertQA: 59.2,
    LFQA: 83.1,
    RAGTruth: 85.3,
    color: "#84cc16",
    link: "https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4",
  },
  {
    model: "GPT-3.5-Turbo",
    Size: "-",
    CNN: 63.2,
    XSum: 72.4,
    MediaS: 66.8,
    MeetB: 73.4,
    WiCE: 68.5,
    REVEAL: 84.7,
    ClaimVerify: 65.2,
    FactCheck: 70.8,
    ExpertQA: 57.2,
    LFQA: 73.8,
    RAGTruth: 75.6,
    color: "#d9f99d",
    link: "https://platform.openai.com/docs/models/gpt-3-5-turbo",
  },
  {
    model: "Claude-3.5 Sonnet",
    Size: "-",
    CNN: 67.6,
    XSum: 75.1,
    MediaS: 73.4,
    MeetB: 84.6,
    WiCE: 77.7,
    REVEAL: 89.1,
    ClaimVerify: 71.4,
    FactCheck: 77.8,
    ExpertQA: 60.9,
    LFQA: 85.6,
    RAGTruth: 86.1,
    color: "#fb923c",
    link: "https://www.anthropic.com/news/claude-3-5-sonnet",
  },
  {
    model: "Claude-3 Opus",
    Size: "-",
    Average: 74.8,
    CNN: 65.2,
    XSum: 72.4,
    MediaS: 74.1,
    MeetB: 82.4,
    WiCE: 75.0,
    REVEAL: 83.8,
    ClaimVerify: 69.3,
    FactCheck: 78.8,
    ExpertQA: 58.8,
    LFQA: 81.6,
    RAGTruth: 81.8,
    color: "#da9757",
    link: "https://www.anthropic.com/news/claude-3-family",
  },
  {
    model: "Claude-2.1",
    Size: "-",
    CNN: 59.9,
    XSum: 66.4,
    MediaS: 69.2,
    MeetB: 72.3,
    WiCE: 64.3,
    REVEAL: 88.2,
    ClaimVerify: 69.7,
    FactCheck: 79.3,
    ExpertQA: 59.8,
    LFQA: 78.2,
    RAGTruth: 75.0,
    color: "#f6c290",
    link: "https://www.anthropic.com/news/claude-2-1",
  },
  {
    model: "Gemini-Pro",
    Size: "-",
    CNN: 49.4,
    XSum: 60.6,
    MediaS: 63.8,
    MeetB: 65.8,
    WiCE: 65.8,
    REVEAL: 85.5,
    ClaimVerify: 61.8,
    FactCheck: 76.8,
    ExpertQA: 56.8,
    LFQA: 75.9,
    RAGTruth: 57.6,
    link: "https://deepmind.google/technologies/gemini/pro/",
  },
  {
    model: "PaLM2-Bison",
    Size: "-",
    CNN: 52.4,
    XSum: 59.0,
    MediaS: 68.3,
    MeetB: 73.6,
    WiCE: 63.4,
    REVEAL: 84.2,
    ClaimVerify: 60.5,
    FactCheck: 76.4,
    ExpertQA: 56.6,
    LFQA: 71.4,
    RAGTruth: 61.6,
    link: "https://blog.google/technology/ai/google-palm-2-ai-large-language-model/",
  },
  {
    model: "Mistral-Large 2",
    Size: "123B",
    CNN: 64.8,
    XSum: 74.7,
    MediaS: 69.6,
    MeetB: 84.2,
    WiCE: 80.3,
    REVEAL: 87.7,
    ClaimVerify: 71.8,
    FactCheck: 74.5,
    ExpertQA: 60.8,
    LFQA: 87.0,
    RAGTruth: 85.9,
    color: "#9d174d",
    link: "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
  },
  {
    model: "Mistral-Large",
    Size: "-",
    CNN: 58.4,
    XSum: 76.3,
    MediaS: 67.3,
    MeetB: 78.9,
    WiCE: 76.6,
    REVEAL: 88.4,
    ClaimVerify: 67.6,
    FactCheck: 79.0,
    ExpertQA: 60.0,
    LFQA: 81.7,
    RAGTruth: 81.7,
    color: "#db2777",
    link: "https://mistral.ai/news/mistral-large/",
  },
  {
    model: "Mixtral-8x22B",
    Size: "176B",
    CNN: 57.3,
    XSum: 70.3,
    MediaS: 69.0,
    MeetB: 78.5,
    WiCE: 69.5,
    REVEAL: 85.8,
    ClaimVerify: 63.8,
    FactCheck: 79.5,
    ExpertQA: 57.4,
    LFQA: 76.5,
    RAGTruth: 78.8,
    color: "#f472b6",
    link: "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1",
  },
  {
    model: "Mistral-8x7B",
    Size: "56B",
    CNN: 55.0,
    XSum: 65.5,
    MediaS: 68.5,
    MeetB: 73.3,
    WiCE: 63.8,
    REVEAL: 80.8,
    ClaimVerify: 64.3,
    FactCheck: 75.1,
    ExpertQA: 56.3,
    LFQA: 70.8,
    RAGTruth: 68.1,
    color: "#f9a8d4",
    link: "https://mistral.ai/technology/#models",
  },
  {
    model: "Qwen2.5-72B-Instruct",
    Size: "72B",
    CNN: 63.6,
    XSum: 73.0,
    MediaS: 71.9,
    MeetB: 80.4,
    WiCE: 80.2,
    REVEAL: 88.9,
    ClaimVerify: 70.0,
    FactCheck: 77.0,
    ExpertQA: 60.1,
    LFQA: 84.3,
    RAGTruth: 81.9,
    color: "#1a5419",
    link: "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct",
  },
  {
    model: "Qwen2.5-7B-Instruct",
    Size: "7B",
    CNN: 54.6,
    XSum: 69.9,
    MediaS: 71.5,
    MeetB: 75.2,
    WiCE: 76.3,
    REVEAL: 86.9,
    ClaimVerify: 70.1,
    FactCheck: 74.9,
    ExpertQA: 61.0,
    LFQA: 84.2,
    RAGTruth: 76.5,
    color: "#34a832",
    link: "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
  },
  {
    model: "Qwen2.5-0.5B-Instruct",
    Size: "0.5B",
    CNN: 55.5,
    XSum: 62.9,
    MediaS: 60.0,
    MeetB: 64.7,
    WiCE: 65.9,
    REVEAL: 85.8,
    ClaimVerify: 61.4,
    FactCheck: 68.9,
    ExpertQA: 56.8,
    LFQA: 72.4,
    RAGTruth: 51.6,
    color: "#7c967b",
    link: "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct",
  },
  {
    model: "Llama-3.2-3B-Instruct",
    Size: "3B",
    CNN: 51.5,
    XSum: 60.5,
    MediaS: 53.1,
    MeetB: 52.5,
    WiCE: 58.5,
    REVEAL: 81.9,
    ClaimVerify: 62.3,
    FactCheck: 62.6,
    ExpertQA: 55.8,
    LFQA: 58.5,
    RAGTruth: 63.0,
    color: "#bfdbfd",
    link: "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
  },
  {
    model: "Llama-3.2-1B-Instruct",
    Size: "1B",
    CNN: 50.1,
    XSum: 50.9,
    MediaS: 50.0,
    MeetB: 50.2,
    WiCE: 49.7,
    REVEAL: 50.4,
    ClaimVerify: 50.5,
    FactCheck: 50.2,
    ExpertQA: 49.9,
    LFQA: 50.1,
    RAGTruth: 50.9,
    color: "#dfedfe",
    link: "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
  },
  {
    model: "Llama-3.1-405B-Instruct",
    Size: "405B",
    CNN: 64.8,
    XSum: 75.1,
    MediaS: 68.6,
    MeetB: 81.2,
    WiCE: 71.8,
    REVEAL: 86.4,
    ClaimVerify: 67.5,
    FactCheck: 79.4,
    ExpertQA: 58.5,
    LFQA: 81.9,
    RAGTruth: 82.9,
    color: "#1d4ed8",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct",
  },
  {
    model: "Llama-3.1-70B-Instruct",
    Size: "70B",
    CNN: 65.7,
    XSum: 72.5,
    MediaS: 72.9,
    MeetB: 81.0,
    WiCE: 73.9,
    REVEAL: 86.4,
    ClaimVerify: 70.3,
    FactCheck: 78.6,
    ExpertQA: 58.5,
    LFQA: 83.8,
    RAGTruth: 83.0,
    color: "#3b82f6",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct",
  },
  {
    model: "Llama-3.1-8B-Instruct",
    Size: "8B",
    CNN: 54.7,
    XSum: 68.5,
    MediaS: 71.1,
    MeetB: 75.5,
    WiCE: 72.0,
    REVEAL: 83.5,
    ClaimVerify: 66.5,
    FactCheck: 72.3,
    ExpertQA: 57.8,
    LFQA: 77.5,
    RAGTruth: 73.6,
    color: "#60a5fa",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
  },
  {
    model: "Llama-3-70B-Instruct",
    Size: "70B",
    CNN: 63.7,
    XSum: 70.2,
    MediaS: 71.5,
    MeetB: 80.6,
    WiCE: 74.4,
    REVEAL: 85.9,
    ClaimVerify: 67.8,
    FactCheck: 76.2,
    ExpertQA: 57.8,
    LFQA: 82.4,
    RAGTruth: 80.6,
    color: "#93c5fd",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
  },
  {
    model: "InternLM2.5-7B-chat",
    Size: "7B",
    CNN: 58.4,
    XSum: 57.9,
    MediaS: 62.5,
    MeetB: 68.0,
    WiCE: 63.7,
    REVEAL: 77.3,
    ClaimVerify: 59.8,
    FactCheck: 67.5,
    ExpertQA: 55.2,
    LFQA: 64.9,
    RAGTruth: 64.0,
    link: "https://huggingface.co/internlm/internlm2_5-7b-chat",
  },
  {
    model: "T5-NLI-Mixed",
    Size: "11B",
    CNN: 54.6,
    XSum: 52.3,
    MediaS: 59.1,
    MeetB: 55.3,
    WiCE: 55.3,
    REVEAL: 87.2,
    ClaimVerify: 59.5,
    FactCheck: 69.0,
    ExpertQA: 55.6,
    LFQA: 61.8,
    RAGTruth: 62.5,
    link: "https://huggingface.co/google/t5_xxl_true_nli_mixture",
  },
  {
    model: "DAE",
    Size: "0.1B",
    CNN: 50.8,
    XSum: 59.1,
    MediaS: 65.1,
    MeetB: 69.5,
    WiCE: 58.5,
    REVEAL: 81.3,
    ClaimVerify: 64.0,
    FactCheck: 72.5,
    ExpertQA: 56.2,
    LFQA: 72.2,
    RAGTruth: 65.3,
    link: "https://github.com/tagoyal/factuality-datasets",
  },
  {
    model: "SummaC-ZS",
    Size: "0.1B",
    CNN: 51.1,
    XSum: 61.5,
    MediaS: 69.5,
    MeetB: 71.0,
    WiCE: 62.8,
    REVEAL: 85.3,
    ClaimVerify: 69.7,
    FactCheck: 75.2,
    ExpertQA: 55.2,
    LFQA: 77.6,
    RAGTruth: 65.6,
    link: "https://github.com/tingofurro/summac/",
  },
  {
    model: "SummaC-CV",
    Size: "0.1B",
    CNN: 65.2,
    XSum: 54.5,
    MediaS: 63.7,
    MeetB: 62.8,
    WiCE: 54.3,
    REVEAL: 67.7,
    ClaimVerify: 70.9,
    FactCheck: 53.4,
    ExpertQA: 54.9,
    LFQA: 62.1,
    RAGTruth: 61.7,
    link: "https://github.com/tingofurro/summac/",
  },
  {
    model: "AlignScore",
    Size: "0.4B",
    CNN: 52.4,
    XSum: 71.4,
    MediaS: 69.2,
    MeetB: 72.6,
    WiCE: 66.0,
    REVEAL: 85.3,
    ClaimVerify: 69.6,
    FactCheck: 74.3,
    ExpertQA: 58.3,
    LFQA: 84.5,
    RAGTruth: 71.7,
    link: "https://github.com/yuh-zha/AlignScore",
  },
  {
    model: "MiniCheck-RoBERTa-L",
    Size: "0.4B",
    CNN: 63.7,
    XSum: 70.8,
    MediaS: 71.9,
    MeetB: 75.9,
    WiCE: 67.6,
    REVEAL: 88.8,
    ClaimVerify: 77.4,
    FactCheck: 73.3,
    ExpertQA: 57.4,
    LFQA: 84.4,
    RAGTruth: 77.2,
    color: "#ddd6fe",
    link: "https://huggingface.co/lytang/MiniCheck-RoBERTa-Large",
  },
  {
    model: "MiniCheck-DeBERTa-L",
    Size: "0.4B",
    CNN: 64.2,
    XSum: 71.0,
    MediaS: 69.3,
    MeetB: 72.7,
    WiCE: 69.4,
    REVEAL: 87.3,
    ClaimVerify: 75.6,
    FactCheck: 73.0,
    ExpertQA: 58.9,
    LFQA: 83.9,
    RAGTruth: 78.8,
    color: "#ede9fe",
    link: "https://huggingface.co/lytang/MiniCheck-DeBERTa-v3-Large",
  },
  {
    model: "MiniCheck-Flan-T5-L",
    Size: "0.8B",
    CNN: 69.9,
    XSum: 74.3,
    MediaS: 73.6,
    MeetB: 77.3,
    WiCE: 72.2,
    REVEAL: 86.2,
    ClaimVerify: 74.6,
    FactCheck: 74.7,
    ExpertQA: 59.0,
    LFQA: 85.2,
    RAGTruth: 78.0,
    color: "#a78bfa",
    link: "https://huggingface.co/lytang/MiniCheck-Flan-T5-Large",
  },
  {
    model: "Bespoke-Minicheck-7B",
    Size: "7B",
    CNN: 65.5,
    XSum: 77.8,
    MediaS: 76.0,
    MeetB: 78.3,
    WiCE: 83.0,
    REVEAL: 88.0,
    ClaimVerify: 75.3,
    FactCheck: 77.7,
    ExpertQA: 59.2,
    LFQA: 86.7,
    RAGTruth: 84.0,
    color: "#6d28d9",
    link: "https://huggingface.co/bespokelabs/Bespoke-Minicheck-7B",
  },
  {
    model: "HHEM-2.1",
    Size: "0.1B",
    CNN: 62.8,
    XSum: 67.8,
    MediaS: 67.2,
    MeetB: 71.0,
    WiCE: 75.9,
    REVEAL: 86.6,
    ClaimVerify: 73.9,
    FactCheck: 71.8,
    ExpertQA: 58.4,
    LFQA: 84.1,
    RAGTruth: 69.9,
    color: "#c82506",
    link: "https://huggingface.co/vectara/hallucination_evaluation_model",
  },
];
