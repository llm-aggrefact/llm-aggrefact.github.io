import { ModelData } from "@/components/Leaderboard";

export const scoresData: ModelData[] = [
  {
    model: "gpt-4o-mini-2024-07-18",
    Size: "-",
    CNN: 61.8,
    XSum: 73.6,
    MediaS: 71.3,
    MeetB: 79.7,
    Wice: 76.3,
    REVEAL: 85.8,
    "Claim Verify": 69.8,
    "Fact Check": 76.0,
    "Expert QA": 58.3,
    LFQA: 80.3,
    "RAG Truth": 81.6,
    color: "#ddd6fe",
    link: "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
  },
  {
    model: "gpt-4o-2024-05-13",
    Size: "-",
    CNN: 68.1,
    XSum: 76.8,
    MediaS: 71.4,
    MeetB: 79.8,
    Wice: 78.5,
    REVEAL: 86.5,
    "Claim Verify": 69.0,
    "Fact Check": 77.5,
    "Expert QA": 59.6,
    LFQA: 83.6,
    "RAG Truth": 84.3,
    color: "#6d28d9",
    link: "https://platform.openai.com/docs/models/gpt-4o",
  },
  {
    model: "gpt-4-turbo-preview",
    Size: "-",
    CNN: 66.7,
    XSum: 76.5,
    MediaS: 71.4,
    MeetB: 79.9,
    Wice: 80.4,
    REVEAL: 87.8,
    "Claim Verify": 67.6,
    "Fact Check": 79.9,
    "Expert QA": 59.2,
    LFQA: 83.1,
    "RAG Truth": 85.3,
    color: "#a78bfa",
    link: "https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4",
  },
  {
    model: "GPT-3.5-Turbo",
    Size: "-",
    CNN: 63.2,
    XSum: 72.4,
    MediaS: 66.8,
    MeetB: 73.4,
    Wice: 68.5,
    REVEAL: 84.7,
    "Claim Verify": 65.2,
    "Fact Check": 70.8,
    "Expert QA": 57.2,
    LFQA: 73.8,
    "RAG Truth": 75.6,
    color: "#ede9fe",
    link: "https://platform.openai.com/docs/models/gpt-3-5-turbo",
  },
  {
    model: "Claude-3 Opus",
    Size: "-",
    Average: 74.8,
    CNN: 65.2,
    XSum: 72.4,
    MediaS: 74.1,
    MeetB: 82.4,
    Wice: 75.0,
    REVEAL: 83.8,
    "Claim Verify": 69.3,
    "Fact Check": 78.8,
    "Expert QA": 58.8,
    LFQA: 81.6,
    "RAG Truth": 81.8,
    color: "#c2410c",
    link: "https://www.anthropic.com/news/claude-3-family",
  },
  {
    model: "Claude-2.1",
    Size: "-",
    CNN: 59.9,
    XSum: 66.4,
    MediaS: 69.2,
    MeetB: 72.3,
    Wice: 64.3,
    REVEAL: 88.2,
    "Claim Verify": 69.7,
    "Fact Check": 79.3,
    "Expert QA": 59.8,
    LFQA: 78.2,
    "RAG Truth": 75.0,
    color: "#fb923c",
    link: "https://www.anthropic.com/news/claude-2-1",
  },
  {
    model: "Gemini-Pro",
    Size: "-",
    CNN: 49.4,
    XSum: 60.6,
    MediaS: 63.8,
    MeetB: 65.8,
    Wice: 65.8,
    REVEAL: 85.5,
    "Claim Verify": 61.8,
    "Fact Check": 76.8,
    "Expert QA": 56.8,
    LFQA: 75.9,
    "RAG Truth": 57.6,
    link: "https://deepmind.google/technologies/gemini/pro/",
  },
  {
    model: "PaLM2-Bison",
    Size: "-",
    CNN: 52.4,
    XSum: 59.0,
    MediaS: 68.3,
    MeetB: 73.6,
    Wice: 63.4,
    REVEAL: 84.2,
    "Claim Verify": 60.5,
    "Fact Check": 76.4,
    "Expert QA": 56.6,
    LFQA: 71.4,
    "RAG Truth": 61.6,
    link: "https://blog.google/technology/ai/google-palm-2-ai-large-language-model/",
  },
  {
    model: "Mistral-Large 2",
    Size: "123B",
    CNN: 64.8,
    XSum: 74.7,
    MediaS: 69.6,
    MeetB: 84.2,
    Wice: 80.3,
    REVEAL: 87.7,
    "Claim Verify": 71.8,
    "Fact Check": 74.5,
    "Expert QA": 60.8,
    LFQA: 87.0,
    "RAG Truth": 85.9,
    color: "#db2777",
    link: "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
  },
  {
    model: "Mistral-Large",
    Size: "-",
    CNN: 58.4,
    XSum: 76.3,
    MediaS: 67.3,
    MeetB: 78.9,
    Wice: 76.6,
    REVEAL: 88.4,
    "Claim Verify": 67.6,
    "Fact Check": 79.0,
    "Expert QA": 60.0,
    LFQA: 81.7,
    "RAG Truth": 81.7,
    color: "#f472b6",
    link: "https://mistral.ai/news/mistral-large/",
  },
  {
    model: "Mixtral-8x22B",
    Size: "176B",
    CNN: 57.3,
    XSum: 70.3,
    MediaS: 69.0,
    MeetB: 78.5,
    Wice: 69.5,
    REVEAL: 85.8,
    "Claim Verify": 63.8,
    "Fact Check": 79.5,
    "Expert QA": 57.4,
    LFQA: 76.5,
    "RAG Truth": 78.8,
    color: "#9d174d",
    link: "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1",
  },
  {
    model: "Mistral-8x7B",
    Size: "56B",
    CNN: 55.0,
    XSum: 65.5,
    MediaS: 68.5,
    MeetB: 73.3,
    Wice: 63.8,
    REVEAL: 80.8,
    "Claim Verify": 64.3,
    "Fact Check": 75.1,
    "Expert QA": 56.3,
    LFQA: 70.8,
    "RAG Truth": 68.1,
    color: "#f9a8d4",
    link: "https://mistral.ai/technology/#models",
  },
  {
    model: "Llama-3.1-405B-Instruct",
    Size: "405B",
    CNN: 64.8,
    XSum: 75.1,
    MediaS: 68.6,
    MeetB: 81.2,
    Wice: 71.8,
    REVEAL: 86.4,
    "Claim Verify": 67.5,
    "Fact Check": 79.4,
    "Expert QA": 58.5,
    LFQA: 81.9,
    "RAG Truth": 82.9,
    color: "#1d4ed8",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct",
  },
  {
    model: "Llama-3.1-70B-Instruct",
    Size: "70B",
    CNN: 65.7,
    XSum: 72.5,
    MediaS: 72.9,
    MeetB: 81.0,
    Wice: 73.9,
    REVEAL: 86.4,
    "Claim Verify": 70.3,
    "Fact Check": 78.6,
    "Expert QA": 58.5,
    LFQA: 83.8,
    "RAG Truth": 83.0,
    color: "#3b82f6",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct",
  },
  {
    model: "Llama-3.1-8B-Instruct",
    Size: "8B",
    CNN: 54.7,
    XSum: 68.5,
    MediaS: 71.1,
    MeetB: 75.5,
    Wice: 72.0,
    REVEAL: 83.5,
    "Claim Verify": 66.5,
    "Fact Check": 72.3,
    "Expert QA": 57.8,
    LFQA: 77.5,
    "RAG Truth": 73.6,
    color: "#60a5fa",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
  },
  {
    model: "Llama-3-70B-Instruct",
    Size: "70B",
    CNN: 63.7,
    XSum: 70.2,
    MediaS: 71.5,
    MeetB: 80.6,
    Wice: 74.4,
    REVEAL: 85.9,
    "Claim Verify": 67.8,
    "Fact Check": 76.2,
    "Expert QA": 57.8,
    LFQA: 82.4,
    "RAG Truth": 80.6,
    color: "#93c5fd",
    link: "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
  },
  {
    model: "InternLM2.5-7B-chat",
    Size: "7B",
    CNN: 58.4,
    XSum: 57.9,
    MediaS: 62.5,
    MeetB: 68.0,
    Wice: 63.7,
    REVEAL: 77.3,
    "Claim Verify": 59.8,
    "Fact Check": 67.5,
    "Expert QA": 55.2,
    LFQA: 64.9,
    "RAG Truth": 64.0,
    link: "https://huggingface.co/internlm/internlm2_5-7b-chat",
  },
  {
    model: "T5-NLI-Mixed",
    Size: "11B",
    CNN: 54.6,
    XSum: 52.3,
    MediaS: 59.1,
    MeetB: 55.3,
    Wice: 55.3,
    REVEAL: 87.2,
    "Claim Verify": 59.5,
    "Fact Check": 69.0,
    "Expert QA": 55.6,
    LFQA: 61.8,
    "RAG Truth": 62.5,
    link: "https://huggingface.co/google/t5_xxl_true_nli_mixture",
  },
  {
    model: "DAE",
    Size: "0.1B",
    CNN: 50.8,
    XSum: 59.1,
    MediaS: 65.1,
    MeetB: 69.5,
    Wice: 58.5,
    REVEAL: 81.3,
    "Claim Verify": 64.0,
    "Fact Check": 72.5,
    "Expert QA": 56.2,
    LFQA: 72.2,
    "RAG Truth": 65.3,
    link: "https://github.com/tagoyal/factuality-datasets",
  },
  {
    model: "SummaC-ZS",
    Size: "0.1B",
    CNN: 51.1,
    XSum: 61.5,
    MediaS: 69.5,
    MeetB: 71.0,
    Wice: 62.8,
    REVEAL: 85.3,
    "Claim Verify": 69.7,
    "Fact Check": 75.2,
    "Expert QA": 55.2,
    LFQA: 77.6,
    "RAG Truth": 65.6,
    link: "https://github.com/tingofurro/summac/",
  },
  {
    model: "SummaC-CV",
    Size: "0.1B",
    CNN: 65.2,
    XSum: 54.5,
    MediaS: 63.7,
    MeetB: 62.8,
    Wice: 54.3,
    REVEAL: 67.7,
    "Claim Verify": 70.9,
    "Fact Check": 53.4,
    "Expert QA": 54.9,
    LFQA: 62.1,
    "RAG Truth": 61.7,
    link: "https://github.com/tingofurro/summac/",
  },
  {
    model: "AlignScore",
    Size: "0.4B",
    CNN: 52.4,
    XSum: 71.4,
    MediaS: 69.2,
    MeetB: 72.6,
    Wice: 66.0,
    REVEAL: 85.3,
    "Claim Verify": 69.6,
    "Fact Check": 74.3,
    "Expert QA": 58.3,
    LFQA: 84.5,
    "RAG Truth": 71.7,
    link: "https://github.com/yuh-zha/AlignScore",
  },
  {
    model: "MiniCheck-RoBERTa-L",
    Size: "0.4B",
    CNN: 63.7,
    XSum: 70.8,
    MediaS: 71.9,
    MeetB: 75.9,
    Wice: 67.6,
    REVEAL: 88.8,
    "Claim Verify": 77.4,
    "Fact Check": 73.3,
    "Expert QA": 57.4,
    LFQA: 84.4,
    "RAG Truth": 77.2,
    color: "#d9f99d",
    link: "https://huggingface.co/lytang/MiniCheck-RoBERTa-Large",
  },
  {
    model: "MiniCheck-DeBERTa-L",
    Size: "0.4B",
    CNN: 64.2,
    XSum: 71.0,
    MediaS: 69.3,
    MeetB: 72.7,
    Wice: 69.4,
    REVEAL: 87.3,
    "Claim Verify": 75.6,
    "Fact Check": 73.0,
    "Expert QA": 58.9,
    LFQA: 83.9,
    "RAG Truth": 78.8,
    color: "#ecfccb",
    link: "https://huggingface.co/lytang/MiniCheck-DeBERTa-v3-Large",
  },
  {
    model: "MiniCheck-Flan-T5-L",
    Size: "0.8B",
    CNN: 69.9,
    XSum: 74.3,
    MediaS: 73.6,
    MeetB: 77.3,
    Wice: 72.2,
    REVEAL: 86.2,
    "Claim Verify": 74.6,
    "Fact Check": 74.7,
    "Expert QA": 59.0,
    LFQA: 85.2,
    "RAG Truth": 78.0,
    color: "#bef264",
    link: "https://huggingface.co/lytang/MiniCheck-Flan-T5-Large",
  },
  {
    model: "Llama3.1-Bespoke-MiniCheck-8B",
    Size: "8B",
    CNN: 62.5,
    XSum: 75.2,
    MediaS: 70.6,
    MeetB: 76.1,
    Wice: 84.0,
    REVEAL: 88.4,
    "Claim Verify": 74.1,
    "Fact Check": 78.8,
    "Expert QA": 59.3,
    LFQA: 85.5,
    "RAG Truth": 83.5,
    color: "#a3e635",
    link: "https://huggingface.co/bespokelabs/Llama3.1-Bespoke-MiniCheck-8B",
  },
  {
    model: "Bespoke-Minicheck-7B",
    Size: "7B",
    Average: 77.1,
    CNN: 66.1,
    XSum: 76.6,
    MediaS: 75.8,
    MeetB: 76.1,
    Wice: 81.9,
    REVEAL: 88.2,
    "Claim Verify": 76.0,
    "Fact Check": 77.0,
    "Expert QA": 58.8,
    LFQA: 87.9,
    "RAG Truth": 84.7,
    color: "#84cc16",
    link: "https://huggingface.co/bespokelabs/Bespoke-Minicheck-7B",
  },
];
